{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n",
      "this really is a test.\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a test.\")\n",
    "print(\"this really is a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  37.89kB\n",
      "Step 1/25 : FROM ubuntu:16.04\n",
      " ---> 2a697363a870\n",
      "Step 2/25 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> 28b035530484\n",
      "Step 3/25 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 609870c850b5\n",
      "Step 4/25 : RUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py\n",
      " ---> Using cache\n",
      " ---> 69c32e7c3c31\n",
      "Step 5/25 : RUN pip install numpy\n",
      " ---> Using cache\n",
      " ---> e11ac61f9717\n",
      "Step 6/25 : RUN pip install scipy\n",
      " ---> Using cache\n",
      " ---> 58d65875a34d\n",
      "Step 7/25 : RUN pip install scikit-learn\n",
      " ---> Using cache\n",
      " ---> cd4ee06bf927\n",
      "Step 8/25 : RUN pip install pandas\n",
      " ---> Using cache\n",
      " ---> 62f3023e03b9\n",
      "Step 9/25 : RUN pip install flask\n",
      " ---> Using cache\n",
      " ---> 25381709b7ac\n",
      "Step 10/25 : RUN pip install gevent\n",
      " ---> Using cache\n",
      " ---> 130187e4c92e\n",
      "Step 11/25 : RUN pip install gunicorn\n",
      " ---> Using cache\n",
      " ---> 39b211ccb047\n",
      "Step 12/25 : RUN pip install tensorflow\n",
      " ---> Using cache\n",
      " ---> 15fd0ec91635\n",
      "Step 13/25 : RUN pip install keras\n",
      " ---> Using cache\n",
      " ---> 1183e6ccd3a5\n",
      "Step 14/25 : RUN pip install gensim\n",
      " ---> Using cache\n",
      " ---> cbc118495d86\n",
      "Step 15/25 : RUN pip install tqdm\n",
      " ---> Using cache\n",
      " ---> 90a4affe8bf2\n",
      "Step 16/25 : RUN pip install nltk\n",
      " ---> Using cache\n",
      " ---> 27b61cb13db2\n",
      "Step 17/25 : RUN pip install emoji\n",
      " ---> Using cache\n",
      " ---> 929896ec1b8f\n",
      "Step 18/25 : RUN pip install sagemaker\n",
      " ---> Using cache\n",
      " ---> 9a57aabef32d\n",
      "Step 19/25 : RUN (cd /usr/local/lib/python2.7/dist-packages/scipy/.libs; rm *; ln ../../numpy/.libs/* .)\n",
      " ---> Using cache\n",
      " ---> f251b8254981\n",
      "Step 20/25 : RUN rm -rf /root/.cache\n",
      " ---> Using cache\n",
      " ---> 92c6905a4b42\n",
      "Step 21/25 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> bcba5d62f06a\n",
      "Step 22/25 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> abc50da127ed\n",
      "Step 23/25 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 8303c32976ef\n",
      "Step 24/25 : COPY nn /opt/program\n",
      " ---> Using cache\n",
      " ---> c60643090793\n",
      "Step 25/25 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> bba645afbb94\n",
      "Successfully built bba645afbb94\n",
      "Successfully tagged my-test-nn:latest\n",
      "The push refers to repository [227919005974.dkr.ecr.us-east-2.amazonaws.com/my-test-nn]\n",
      "8b4b00752b9f: Preparing\n",
      "5dd14791a7bc: Preparing\n",
      "eb191421be90: Preparing\n",
      "62fc2e49f926: Preparing\n",
      "516d2b78e1e1: Preparing\n",
      "7e9600af186a: Preparing\n",
      "2b3a02e4734c: Preparing\n",
      "ec7328931242: Preparing\n",
      "bcbf3a0d46e1: Preparing\n",
      "3d63ed95b99c: Preparing\n",
      "a92abc01f2cf: Preparing\n",
      "ffad6563bbc2: Preparing\n",
      "2f1466081bf9: Preparing\n",
      "024bec94a520: Preparing\n",
      "42f83e119601: Preparing\n",
      "d4d8b88af9f2: Preparing\n",
      "ce1fdd211bb4: Preparing\n",
      "b063813a89e9: Preparing\n",
      "011a5a1d56db: Preparing\n",
      "4c54072a5034: Preparing\n",
      "49652298c779: Preparing\n",
      "e15278fcccca: Preparing\n",
      "739482a9723d: Preparing\n",
      "2b3a02e4734c: Waiting\n",
      "ec7328931242: Waiting\n",
      "bcbf3a0d46e1: Waiting\n",
      "3d63ed95b99c: Waiting\n",
      "a92abc01f2cf: Waiting\n",
      "ffad6563bbc2: Waiting\n",
      "2f1466081bf9: Waiting\n",
      "024bec94a520: Waiting\n",
      "42f83e119601: Waiting\n",
      "d4d8b88af9f2: Waiting\n",
      "ce1fdd211bb4: Waiting\n",
      "b063813a89e9: Waiting\n",
      "011a5a1d56db: Waiting\n",
      "4c54072a5034: Waiting\n",
      "49652298c779: Waiting\n",
      "e15278fcccca: Waiting\n",
      "739482a9723d: Waiting\n",
      "7e9600af186a: Waiting\n",
      "8b4b00752b9f: Layer already exists\n",
      "516d2b78e1e1: Layer already exists\n",
      "eb191421be90: Layer already exists\n",
      "5dd14791a7bc: Layer already exists\n",
      "62fc2e49f926: Layer already exists\n",
      "bcbf3a0d46e1: Layer already exists\n",
      "ec7328931242: Layer already exists\n",
      "7e9600af186a: Layer already exists\n",
      "3d63ed95b99c: Layer already exists\n",
      "2b3a02e4734c: Layer already exists\n",
      "a92abc01f2cf: Layer already exists\n",
      "ffad6563bbc2: Layer already exists\n",
      "2f1466081bf9: Layer already exists\n",
      "024bec94a520: Layer already exists\n",
      "d4d8b88af9f2: Layer already exists\n",
      "ce1fdd211bb4: Layer already exists\n",
      "42f83e119601: Layer already exists\n",
      "b063813a89e9: Layer already exists\n",
      "011a5a1d56db: Layer already exists\n",
      "739482a9723d: Layer already exists\n",
      "49652298c779: Layer already exists\n",
      "4c54072a5034: Layer already exists\n",
      "e15278fcccca: Layer already exists\n",
      "latest: digest: sha256:abaa997d70268606104770a883034391ac5470a9c1c8962e37c1a22ae860f34d size: 5159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=my-test-nn\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x nn/train\n",
    "chmod +x nn/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://f404466-emr-poc/data/tweets/tweets3.csv ./CNN/data/tweets2.csv\n",
    "#!mkdir /tmp/CNN/train\n",
    "#!aws s3 cp s3://f404466-emr-poc/data/tweets/train/tweets.csv /tmp/CNN/train/tweets.csv\n",
    "#!aws s3 cp s3://f404466-emr-poc/data/tweets/train/tweets.csv ./data/train/tweets.csv\n",
    "#!aws s3 cp s3://f404466-emr-poc/data/tweets/test/tweets.csv ./data/test/tweets.csv\n",
    "#!mkdir /tmp/CNN/test\n",
    "#!aws s3 cp s3://f404466-emr-poc/data/tweets/test/tweets.csv /tmp/CNN/test/tweets.csv\n",
    "#!aws s3 cp s3://f404466-emr-poc/data/tweets/tweets3.csv ./CNN/data/tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/tmp/NN/train/tweets.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df=df[df['sentiment']!='MIXED']\n",
    "\n",
    "train = df\n",
    "\n",
    "df = pd.read_csv(\"/tmp/NN/test/tweets.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df=df[df['sentiment']!='MIXED']\n",
    "\n",
    "test = df\n",
    "\n",
    "data = pd.concat([train, test])\n",
    "\n",
    "data.to_csv('data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-29 19:27:51 Starting - Starting the training job...\n",
      "2019-05-29 19:27:53 Starting - Launching requested ML instances......\n",
      "2019-05-29 19:29:00 Starting - Preparing the instances for training...\n",
      "2019-05-29 19:29:46 Downloading - Downloading input data...\n",
      "2019-05-29 19:29:54 Training - Downloading the training image..\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mStarting the training.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse tf.cast instead.\u001b[0m\n",
      "\u001b[31mEpoch 1/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.4743 - acc: 0.7829\u001b[0m\n",
      "\u001b[31mEpoch 2/25\u001b[0m\n",
      "\n",
      "2019-05-29 19:30:26 Training - Training image download completed. Training in progress.\u001b[31m - 6s - loss: 0.3842 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 3/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.3837 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 4/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.3830 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 5/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.3839 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 6/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.3836 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 7/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.3833 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 8/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.3832 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 9/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.3828 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 10/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.3829 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 11/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.3830 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 12/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.3799 - acc: 0.8574\u001b[0m\n",
      "\u001b[31mEpoch 13/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.3302 - acc: 0.8642\u001b[0m\n",
      "\u001b[31mEpoch 14/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.2300 - acc: 0.9100\u001b[0m\n",
      "\u001b[31mEpoch 15/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.1709 - acc: 0.9353\u001b[0m\n",
      "\u001b[31mEpoch 16/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.1437 - acc: 0.9468\u001b[0m\n",
      "\u001b[31mEpoch 17/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.1229 - acc: 0.9552\u001b[0m\n",
      "\u001b[31mEpoch 18/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.1073 - acc: 0.9604\u001b[0m\n",
      "\u001b[31mEpoch 19/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.0977 - acc: 0.9650\u001b[0m\n",
      "\u001b[31mEpoch 20/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.0888 - acc: 0.9670\u001b[0m\n",
      "\u001b[31mEpoch 21/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.0837 - acc: 0.9687\u001b[0m\n",
      "\u001b[31mEpoch 22/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.0780 - acc: 0.9691\u001b[0m\n",
      "\u001b[31mEpoch 23/25\u001b[0m\n",
      "\u001b[31m - 5s - loss: 0.0718 - acc: 0.9710\u001b[0m\n",
      "\u001b[31mEpoch 24/25\u001b[0m\n",
      "\n",
      "2019-05-29 19:32:53 Uploading - Uploading generated training model\u001b[31m - 5s - loss: 0.0692 - acc: 0.9716\u001b[0m\n",
      "\u001b[31mEpoch 25/25\u001b[0m\n",
      "\u001b[31m - 6s - loss: 0.0625 - acc: 0.9749\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python2.7/dist-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\u001b[0m\n",
      "\u001b[31mTraining is complete.\u001b[0m\n",
      "\n",
      "2019-05-29 19:32:59 Completed - Training job completed\n",
      "Billable seconds: 193\n"
     ]
    }
   ],
   "source": [
    "# S3 prefix\n",
    "prefix = 'f404466'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "WORK_DIRECTORY = 'data/train'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY))\n",
    "#data_location = 'file:///tmp/CNN/'\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = f'227919005974.dkr.ecr.us-east-2.amazonaws.com/my-test-nn:latest'\n",
    "#image = f'my-test-nn:latest'\n",
    "\n",
    "#instance_type = 'local'\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "#instance_type = 'ml.m5.2xlarge'\n",
    "\n",
    "classifier = sage.estimator.Estimator(\n",
    "    image, \n",
    "    role = role, \n",
    "    train_instance_count = 1, \n",
    "    train_instance_type = instance_type, \n",
    "    #image_name = image)\n",
    "    output_path=\"s3://f404466-emr-poc/output\",\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# deploy it to an endpoint\n",
    "predictor = classifier.deploy(1, 'ml.m5.2xlarge', endpoint_name ='demo-test-endpoint')\n",
    "\n",
    "# connect to the endpoint\n",
    "predictor = sage.predictor.RealTimePredictor(\n",
    "    'demo-test-endpoint',\n",
    "    sagemaker_session=sess,\n",
    "    content_type=\"text/csv\"\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [\"This is a great movie! This is a great movie! This is a great movie!\",\"POSITIVE\"]\n",
    "test2 = [\"This is a terrible movie!\",\"NEGATIVE\"]\n",
    "test3 = [\"This is a movie!\",\"NEUTRAL\"]\n",
    "\n",
    "inputs = [test1,test2,test3]\n",
    "\n",
    "input_data = \"\\n\".join(\n",
    "    [\",\".join(l) for l in inputs]\n",
    ")\n",
    "input_data\n",
    "#print(input_data)\n",
    "#predictor.predict(input_data)\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet sentiment\n",
      "0  This is a great movie! This is a great movie! ...  POSITIVE\n",
      "1                          This is a terrible movie!  NEGATIVE\n",
      "2                                   This is a movie!   NEUTRAL\n",
      "3 Records in file...\n",
      "Max Length 15\n",
      "Vocab Size 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 5, 4, 1, 2, 3, 5, 4, 1, 2, 3, 5, 4],\n",
       "       [1, 2, 3, 6, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def transform_data(dataset):\n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    tokenizer_obj = Tokenizer()\n",
    "    \n",
    "    tokenizer_obj.fit_on_texts(dataset['tweet'])\n",
    "    \n",
    "    print(\"{} Records in file...\".format(len(dataset)))\n",
    "    \n",
    "    max_length = max([len(s.split()) for s in dataset['tweet']])\n",
    "    \n",
    "    print(\"Max Length {}\".format(max_length))\n",
    "    \n",
    "    vocab_size = len(tokenizer_obj.word_index)+1\n",
    "    \n",
    "    print(\"Vocab Size {}\".format(vocab_size))\n",
    "    \n",
    "    x_train_tokens = tokenizer_obj.texts_to_sequences(dataset['tweet'])\n",
    "    \n",
    "    x_train_pad = pad_sequences(x_train_tokens, maxlen=max_length, padding='post')\n",
    "\n",
    "    #y_train = np.array(dataset.sentiment)\n",
    "    \n",
    "    return x_train_pad #, y_train\n",
    "\n",
    "#import StringIO\n",
    "from io import StringIO\n",
    "s = StringIO(input_data)\n",
    "data = pd.read_csv(s, header=None)\n",
    "data.columns = ['tweet','sentiment']\n",
    "X = transform_data(data)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 584kB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.9)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 27.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 15.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (3.0.5)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Building wheels for collected packages: absl-py, termcolor, gast\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built absl-py termcolor gast\n",
      "Installing collected packages: grpcio, astor, absl-py, termcolor, gast, tensorflow-estimator, markdown, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 grpcio-1.21.1 markdown-3.1.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/demo-test-endpoint in account 227919005974 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-3c74329f941d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#input_data = '0,create new memories in this beautiful home,POSITIVE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/demo-test-endpoint in account 227919005974 for more information."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/test/tweets.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "X_test = (\n",
    "  df.head().values\n",
    ")\n",
    "input_data = \"\\n\".join(\n",
    "    [\",\".join(l) for l in X_test.astype('str').tolist()]\n",
    ")\n",
    "\n",
    "#input_data = '0,create new memories in this beautiful home,POSITIVE'\n",
    "predictor.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "print(sys.version)\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "    \n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/train/tweets.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "X_test = (\n",
    "  df.values\n",
    ")\n",
    "input_data = \"\\n\".join(\n",
    "    [\",\".join(l) for l in X_test.astype('str').tolist()]\n",
    ")\n",
    "\n",
    "s = StringIO(input_data)\n",
    "dataset = pd.read_csv(s, header=None)\n",
    "\n",
    "dataset.columns = ['tweet','sentiment']\n",
    "tokenizer_obj = Tokenizer()   \n",
    "\n",
    "tokenizer_obj.fit_on_texts(dataset['tweet'])    \n",
    "max_length = max([len(s.split()) for s in dataset['tweet']])    \n",
    "vocab_size = len(tokenizer_obj.word_index)+1    \n",
    "x_train_tokens = tokenizer_obj.texts_to_sequences(dataset['tweet'])\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=69)\n",
    "\n",
    "model = load_model('/tmp/tmpxdqiu9as/model/ann-churn.h5')\n",
    "predictions = model.predict(x_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataset['sentiment'])\n",
    "encoded_Y = encoder.transform(dataset['sentiment'])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp2q__gol6/model/ann-churn.h5\n",
      "/tmp/tmp36qpak9f/ann-churn.h5\n",
      "/tmp/tmp4bcw3gu3/ann-churn.h5\n",
      "/tmp/tmp70zyg0a0/ann-churn.h5\n",
      "/tmp/tmp78nuu9mt/ann-churn.h5\n",
      "/tmp/tmp85d6f_4y/model/ann-churn.h5\n",
      "/tmp/tmpgkadbvzb/model/ann-churn.h5\n",
      "/tmp/tmpri_loh0n/model/ann-churn.h5\n",
      "/tmp/tmpusowdard/model/ann-churn.h5\n",
      "/tmp/tmpv1mj8re3/model/ann-churn.h5\n",
      "/tmp/tmpxdqiu9as/model/ann-churn.h5\n",
      "/tmp/tmpytcqdab2/model/ann-churn.h5\n"
     ]
    }
   ],
   "source": [
    "!find /tmp/* -name \"*churn.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[427   1  43 ...   0   0   0]\n",
      " [  5  23 713 ...   0   0   0]\n",
      " [ 13  89  12 ...   0   0   0]\n",
      " ...\n",
      " [179  10 123 ...   0   0   0]\n",
      " [ 13 990 487 ...   0   0   0]\n",
      " [ 21  14  96 ...   0   0   0]]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, input_dim=69, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=69, init='normal', activation='relu'))\n",
    "    model.add(Dense(4, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "df = pd.read_csv(\"./data/train/tweets.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "x_train = df['tweet']\n",
    "y_train = df['sentiment']\n",
    "y_train\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y\n",
    "\n",
    "tokenizer_obj = Tokenizer()   \n",
    "\n",
    "tokenizer_obj.fit_on_texts(x_train)    \n",
    "max_length = max([len(s.split()) for s in x_train])    \n",
    "vocab_size = len(tokenizer_obj.word_index)+1    \n",
    "x_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=69, padding='post')\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size=0.33, random_state=seed)\n",
    "print(x_train_pad)\n",
    "print(dummy_y)\n",
    "estimator.fit(x_train_pad, dummy_y)\n",
    "\n",
    "df = pd.read_csv(\"./data/test/tweets.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "x_test = df['tweet']\n",
    "y_test = df['sentiment']\n",
    "\n",
    "tokenizer_obj.fit_on_texts(x_test)    \n",
    "max_length = max([len(s.split()) for s in x_test])    \n",
    "vocab_size = len(tokenizer_obj.word_index)+1    \n",
    "x_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=69, padding='post')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y\n",
    "\n",
    "predicted = estimator.predict(x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEUTRAL' 'NEGATIVE' 'NEUTRAL' ... 'NEUTRAL' 'NEUTRAL' 'NEUTRAL']\n"
     ]
    }
   ],
   "source": [
    "print(encoder.inverse_transform(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-64381b9a832a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/train/tweets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "raw_data = pd.read_csv(\"./data/train/tweets.csv\")\n",
    "\n",
    "X = raw_data['tweet']\n",
    "\n",
    "x_train = raw_data['tweet']\n",
    "y_train = raw_data['sentiment']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, dummy_y, test_size=0.33, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-showcode": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
